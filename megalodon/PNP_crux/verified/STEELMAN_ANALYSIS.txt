STEELMANNING THE SWITCHING LEMMA - POTENTIAL FAILURE POINTS
============================================================

The Switching-by-Weakness lemma (Theorem 4.2) is THE CRUX.
Here we identify where the Admitted proofs could ACTUALLY FAIL.

============================================================
FAILURE POINT 1: HYPOTHESIS CLASS SIZE |H| - **MOSTLY RESOLVED**
============================================================

The paper says H contains "simple" functions on O(log m) bits.

**INVESTIGATION FINDINGS (from arXiv:2510.08814):**

The paper specifies:
- H consists of ACC⁰ circuits on O(log m) input bits
- "each h_{j,i} is computable in time poly(log m)"
- Training uses t = c₄m samples (LINEAR in m!)

**DETAILED ANALYSIS:**

For ACC⁰ circuits of size s on k bits:
  Number of circuits ≤ (s * k)^{O(s)}

For s = poly(log m) = (log m)^c and k = O(log m):
  |H| ≤ (log m)^{O((c+1) * (log m)^c)}
  log|H| = O((log m)^{c+1} * log log m) = O((log m)^{c+2})

**THE KEY INSIGHT:**

For ERM generalization with error ε:
  Need t ≥ O(log|H| / ε²)

With t = c₄m and log|H| = O((log m)^{c+2}):
  m ≥ (log m)^{c+2} / ε²
  ε² ≥ (log m)^{c+2} / m
  ε ≥ (log m)^{(c+2)/2} / sqrt(m)

As m → ∞:
  ε → 0 because (log m)^k / sqrt(m) → 0 for any constant k

**VERDICT: ERM DOES GENERALIZE** ✓

Even with quasi-polynomial |H| = 2^{(log m)^{c+2}}, the linear
sample count t = Θ(m) is MUCH larger than log|H| = (log m)^{c+2}.

The generalization error ε = O((log m)^k / sqrt(m)) vanishes as m → ∞.

**STATUS: CONCERN MOSTLY RESOLVED** ✓

The only remaining subtlety is whether ε = o(1) is small enough
for the contradiction. The paper needs δ*t > O(log m), and if
ε affects δ, we need ε·m = o(m), which is satisfied.

See hypothesis_class_analysis.mg for formalization.

============================================================
FAILURE POINT 2: CALIBRATION LEMMA ASSUMPTIONS - **ANALYZED**
============================================================

The calibration lemma claims:
  If D has success rho on true labels X,
  then ERM predictor h has success rho - o(1) on X.

This requires surrogate labels Y~ to approximate X.

Y~_{j,i} = majority_{T in family} D(T(instance_j))_i

**FORMALIZED ANALYSIS (see calibration_lemma.mg):**

Key definitions:
  - LocalBits(D,r) = {i ∈ m | D's output at i depends only on r-neighborhood}
  - GlobalBits(D,r) = {i ∈ m | D's output at i depends on distant features}
  - LocalBits ∪ GlobalBits = m (partition)

Case 1: D is "local" at position i
  - T preserves local structure (by SILS sign-invariance)
  - D(T(instance))_i = D(instance)_i (consistent across T's)
  - Majority = D's local output ✓

Case 2: D is "global" at position i
  - T changes global structure
  - D(T(instance))_i varies across T's
  - By symmetry: majority → 1/2
  - Neutrality applies: success rate = 1/2 ✓

**MIXED CASE RESOLUTION:**

If D is partially local/global:
  - LocalBits: ERM finds consistent predictor h
  - GlobalBits: symmetrization averages to 1/2
  - h achieves D's success on LocalBits
  - h achieves 1/2 on GlobalBits (by neutrality)

The contradiction still works because:
  - If |LocalBits| is large: h matches D, but h is local → neutrality → 1/2
  - If |GlobalBits| is large: D itself can't beat 1/2 on those bits

**STATUS: CONCERN RESOLVED** ✓

The local/global dichotomy is EXHAUSTIVE. Either:
  (a) D is local → neutrality kills advantage, or
  (b) D is global → symmetrization kills advantage

See calibration_lemma.mg for formalization.

============================================================
FAILURE POINT 3: WRAPPER ENCODING COST - **RESOLVED**
============================================================

Claimed: K_poly(D.W) <= K_poly(D) + O(log m)

**FORMALIZED ANALYSIS (see wrapper_encoding.mg):**

The wrapper D.W must encode:
  1. The original decoder D: K_poly(D) bits
  2. Seed for symmetrization family: O(log m) bits
  3. "Run ERM on hypothesis space H": O(1) bits
  4. Training/test split specification: O(1) bits (fixed split)
  5. How to access training labels: O(1) bits (recompute instruction)

**KEY INSIGHT: RECOMPUTE, DON'T STORE**

Naive approach (FAILS):
  - Store t*m bits of training labels
  - This is O(m²) bits - NOT O(log m)!

Correct approach (WORKS):
  - Wrapper RECOMPUTES labels on demand
  - Each query recomputes relevant training labels
  - Time per query: O(t * poly(m)) = O(poly(m))
  - K_poly allows poly(m) runtime ✓

**ENCODING COST BREAKDOWN:**

| Component                  | Bits      |
|----------------------------|-----------|
| Original decoder D         | K_poly(D) |
| Symmetrization seed        | O(log m)  |
| ERM algorithm (implicit)   | O(1)      |
| Training split (fixed)     | O(1)      |
| Recompute instruction      | O(1)      |
|----------------------------|-----------|
| TOTAL                      | K_poly(D) + O(log m) ✓ |

**RUNTIME ANALYSIS:**

- Per query: O(t * decoder_time) = O(m * poly(m)) = poly(m)
- K_poly definition allows poly(m) runtime
- Therefore: wrapper is valid in K_poly model ✓

**STATUS: CONCERN RESOLVED** ✓

The wrapper uses on-demand recomputation instead of storage.
This keeps encoding cost at O(log m) while using poly(m) time.

See wrapper_encoding.mg for formalization.

============================================================
FAILURE POINT 4: SIGN-INVARIANCE OF LOCALITY - **RESOLVED**
============================================================

ORIGINAL CONCERN: Does "local view" include sign information?

**RESOLUTION FROM PAPER (arXiv:2510.08814):**

The paper EXPLICITLY addresses this by defining SILS
(Sign-Invariant Local Sketch). Key findings:

1. Local view is sign-invariant BY DEFINITION
   - Features must be invariant under H_m = S_m ⋉ (Z_2)^m
   - The (Z_2)^m factor flips literal signs

2. ADMISSIBLE features (sign-invariant):
   - Variable degree histograms (unsigned)
   - Neighborhood structure IGNORING literal signs
   - Co-occurrence statistics without signs

3. EXCLUDED features (sign-sensitive):
   - Clause parity by signs
   - Features distinguishing positive/negative literals

4. KEY PROPERTY: T_i preserves SILS while toggling X_i
   Quote: "T_i preserves I [σ-algebra from SILS] and toggles X_i"

**STATUS: CONCERN RESOLVED** ✓

See sign_invariance_resolved.mg for formalization.

============================================================
FAILURE POINT 5: THE DELTA CONSTANT
============================================================

The contradiction requires: delta * t > O(log m)

Where delta = per-bit advantage of decoder over random guessing.

CRITICAL QUESTION: Is delta = Omega(1)?

For a perfect decoder: delta = 1/2 (always correct)
For random guessing: delta = 0

The paper claims: if SAT in P, then exists decoder with delta = Omega(1).

But the decoder must find THE SPECIFIC RANDOM WITNESS, not just
any satisfying assignment!

The D_m distribution:
  - Formula F is random 3-CNF
  - Witness w is uniform random satisfying assignment
  - Instance encodes (F, w) in some way

If the encoding makes w easy to recover: delta = Omega(1)
If the encoding hides w: delta might be small

VERDICT: Depends on the specific D_m construction. The paper's
         construction must ensure delta = Omega(1).

============================================================
NEW CRUXES FROM GPT-5.1 PRO ANALYSIS (HIGHER PRIORITY)
============================================================

After deeper analysis, four NEW cruxes have been identified that
are MORE SERIOUS than our earlier concerns. These supersede the
previous assessment.

============================================================
**CRUX #1: T_i DOES NOT PRESERVE u - STRUCTURAL BUG**
============================================================

**Location:** Lemma 4.8, Appendix A.17 (Calibration Lemma)

**The Claim (from paper):**
  For fixed u = (z, a_i, b), the involution T_i bijects
  (X_i=0, Y_i=y) ↔ (X_i=1, Y_i=1-y) WITHOUT CHANGING u.

**The Bug:**
  T_i : (F^h, A, b) → (F^{τ_i h}, A, b ⊕ A·e_i)

  - z (SILS) is preserved: ✓
  - a_i = A·e_i is preserved: ✓
  - b is changed to b ⊕ a_i: ✗ **NOT PRESERVED!**

Therefore u = (z, a_i, b) is NOT preserved by T_i.

**Why This Matters:**
  The "one-line calibration" argument literally says T_i keeps u fixed.
  If u changes, the conditional distribution (X_i, Y_i | u) is NOT
  symmetric under the claimed bijection.

**Possible Fixes:**
  1. Condition on u' = (z, a_i) instead of u = (z, a_i, b)
  2. Argue that adding b doesn't introduce extra bias

**STATUS: CRITICAL BUG - needs resolution**

See crux1_calibration_bug.mg for formalization.

============================================================
**CRUX #2: SPARSIFICATION AFTER UNIQUENESS CONDITIONING**
============================================================

**Location:** Lemma 5.8, Theorem 5.10, Appendix A.13-A.15

**The Gap:**
  - Tree-likeness proofs (Theorem 3.11, A.13) are for UNCONDITIONED
    masked random 3-CNF
  - But D_m is conditioned on Unq(Φ) - unique satisfiability
  - The paper doesn't prove sparsification holds AFTER conditioning

**Why Uniqueness Conditioning Could Break Things:**
  - Uniqueness is a GLOBAL event (depends on whole formula)
  - Uniqueness is RARE in base distribution
  - Conditioning could heavily bias local neighborhoods toward
    "uniquifying" structure (extra cycles, constraints)

**The Missing Lemma:**
  Need to prove: For any fixed radius-r chart C,
    Pr_{Φ∼D_m}[(Φ,i) matches C] ≤ m^{O(1)} · Pr_{uncond}[(Φ,i) matches C]

  If this polynomial distortion doesn't hold, Lemma 5.8 fails under D_m.

**Attack Vectors:**
  1. Empirical: Sample m=30-60, compare local structure between
     unconditional vs USAT-conditioned formulas
  2. Theoretical: Prove local-global approximate independence

**STATUS: CRITICAL GAP - needs investigation**

See crux2_uniqueness_conditioning.mg for formalization.

============================================================
**CRUX #3: FULL SWITCHING-BY-WEAKNESS (Theorem 4.2 / Prop A.5)**
============================================================

**Key Moving Parts:**
  1. W_sym: symmetrization using O(log²(mt)) sign flips
  2. W_ERM: train/test split, plug-in majority rule
  3. Claims exact success preservation (Lemma 4.3, A.2)

**What to Verify:**
  - Independence structure: test blocks are i.i.d. D_m AFTER fixing W
  - Quantifier order: for every P, there exists W with |W| = O(log m)
  - No hidden dependence between training/test given P

**Potential Issue:**
  ERM generalization uses independence of training blocks.
  Need to verify no accidental use of training-test independence
  in a way that becomes false after fixing W for that P.

**STATUS: NEEDS CAREFUL FORMALIZATION**

============================================================
**CRUX #4: K_poly UNION BOUND QUANTIFIERS (Section 6)**
============================================================

**The Counting Issue:**
  - Union bound over all P of length ≤ δt
  - But effective decoders are P' = P ∘ W_ERM
  - Length: |P'| ≤ |P| + O(log m + log t)

**Need to Check:**
  - Either union bound over P' directly (with adjusted δ')
  - Or show composed decoders correspond to ≤ 2^{K+O(1)} base programs

**Potential Bug:**
  Hidden factor of 2 in exponent if P vs P∘W not tracked cleanly.

**STATUS: LOW PRIORITY (likely fixable with constant adjustment)

============================================================
DE-PRIORITIZED CONCERNS (from earlier analysis)
============================================================

The following are now LESS CRITICAL given the v11 structure:

1. "Local ≠ Simple" / ACC⁰ expressiveness
   - Sparsification works for ALL u-measurable rules, not just ACC⁰
   - "No counting over hypothesis class required" (A.15)

2. Polynomial neighborhood size vs K_poly
   - K_poly counts description length, not runtime
   - Radius O(log m) neighborhoods are compatible

3. Raw calibration sample complexity numbers
   - The real issue is structural (Crux #1), not variance

============================================================
OVERALL ASSESSMENT (UPDATED WITH NEW CRUXES)
============================================================

**TWO CRITICAL ISSUES IDENTIFIED:**

1. **CRUX #1: T_i does NOT preserve u = (z, a_i, b)** - CRITICAL
   The calibration lemma claims T_i fixes u, but b → b ⊕ a_i.
   This is a STRUCTURAL BUG in the proof as written.
   See crux1_calibration_bug.mg for formalization.

2. **CRUX #2: Sparsification under uniqueness conditioning** - CRITICAL
   Proofs are for unconditioned 3-CNF, but D_m conditions on Unq(Φ).
   Missing proof that local structure is preserved under conditioning.
   See crux2_uniqueness_conditioning.mg for formalization.

**LOWER PRIORITY ISSUES:**

3. CRUX #3: Full SW formalization (independence structure)
4. CRUX #4: K_poly union bound (likely fixable with constants)

**EARLIER CONCERNS (NOW DE-PRIORITIZED):**

5. Sign-invariance: RESOLVED via SILS ✓
6. |H| size: RESOLVED via t=Θ(m) ✓
7. Wrapper encoding: RESOLVED via recompute ✓
8. ACC⁰ expressiveness: NOT LOAD-BEARING in v11 ✓

**REVISED LIKELIHOOD OF FAILURE: 30-50%**

The GPT-5.1 Pro analysis identified CONCRETE structural issues:
- Crux #1: T_i provably does NOT preserve b component of u
- Crux #2: No proof of local-global independence under conditioning

If Crux #1 or #2 cannot be fixed, the proof fails as written.
The fixes may exist (remove b from u, or prove conditioning lemma),
but they are not in the current paper.

**NEXT STEPS (CRUX-FIRST):**

1. Calibration structural check (Crux #1):
   - Try to prove Pr[X_i=1|u] = Pr[Y_i=1|u] with u=(z,a_i,b)
   - If stuck, search for counterexamples on small instances

2. Uniqueness-conditioning vs sparsification (Crux #2):
   - Prove local neighborhoods retain m^{-Ω(1)} under USAT conditioning
   - Run simulations comparing local structure

3. If #1 and #2 pass, formalize SW and union bound (#3, #4)

**VERIFIED FILES:**
- crux1_calibration_bug.mg (NEW)
- crux2_uniqueness_conditioning.mg (NEW)
- sign_invariance_resolved.mg
- hypothesis_class_analysis.mg
- calibration_lemma.mg
- wrapper_encoding.mg

Sources:
- arXiv:2510.08814 (Goertzel's paper)
- GPT-5.1 Pro analysis (via Mitchell Porter / Shtetl-Optimized)

============================================================
