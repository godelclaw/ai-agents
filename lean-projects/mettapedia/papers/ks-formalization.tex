\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}

\geometry{margin=1in}

% Listings configuration for Lean code
\lstdefinelanguage{Lean}{
  morekeywords={theorem, lemma, def, axiom, class, structure, instance, where, by, import, namespace, open, section, variable, example},
  sensitive=true,
  morecomment=[l]{--},
  morecomment=[s]{/-}{-/},
  morestring=[b]",
}

\lstset{
  language=Lean,
  basicstyle=\small\ttfamily,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{red},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
	  literate=
	    {⊕}{$\oplus$}1
	    {⊗}{$\otimes$}1
	    {→}{$\to$}1
	    {←}{$\gets$}1
	    {↔}{$\leftrightarrow$}1
	    {⟨}{$\langle$}1
	    {⟩}{$\rangle$}1
	    {∀}{$\forall$}1
	    {∃}{$\exists$}1
	    {≤}{$\leq$}1
	    {≥}{$\geq$}1
	    {≠}{$\neq$}1
	    {∈}{$\in$}1
	    {∉}{$\notin$}1
	    {⊥}{$\bot$}1
	    {⊤}{$\top$}1
	    {⊔}{$\sqcup$}1
	    {⊓}{$\sqcap$}1
	    {∧}{$\wedge$}1
	    {∨}{$\vee$}1
	    {¬}{$\neg$}1
	    {ℝ}{$\mathbb{R}$}1
	    {ℕ}{$\mathbb{N}$}1
	    {Θ}{$\Theta$}1
	    {Ψ}{$\Psi$}1
	    {ψ}{$\psi$}1
	    {ζ}{$\zeta$}1
	    {ξ}{$\xi$}1
	    {η}{$\eta$}1
	    {α}{$\alpha$}1
	    {β}{$\beta$}1
	    {γ}{$\gamma$}1
	    {δ}{$\delta$}1
	    {ε}{$\varepsilon$}1
	    {μ}{$\mu$}1
	    {Ω}{$\Omega$}1
	    {≃}{$\simeq$}1
	}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{axiom}[theorem]{Axiom}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\title{Foundations of Inference: A Formal Treatment}
\author{Formalization of Knuth \& Skilling (2012)}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a complete formalization in Lean 4 of Knuth \& Skilling's axiomatic foundations for inference (2012).
Their approach derives the probability calculus, measure theory, and information theory from elementary symmetries, without assuming continuity or differentiability.
Our formalization makes explicit the implicit assumptions in their derivation, provides \textbf{three independent proof paths} for the main representation theorem (grid-based induction, Dedekind cuts, and H\"older embedding), and extends their framework to handle independence and product measures.
We demonstrate K\&S's claim that the framework applies to non-Boolean distributive lattices (chains, open sets of $\mathbb{R}$).
A key finding: the \textbf{NoAnomalousPairs} condition (from classical ordered semigroup theory) is sufficient for the representation theorem, via Eric Paul's formalized H\"older embedding.
\end{abstract}

\tableofcontents

\section{Introduction}

Knuth \& Skilling~\cite{KnuthSkilling2012} presented a foundation for inference that unites and extends the approaches of Cox and Kolmogorov.
Their key innovation is to derive the standard probability calculus from elementary \emph{symmetries} rather than axiomatizing it directly.
The core insight: algebraic symmetries of combination operations uniquely determine quantification rules.

\subsection{The Knuth-Skilling Program}

The K\&S approach proceeds in stages:
\begin{enumerate}
\item \textbf{Symmetries 0--2} (fidelity, monotonicity, associativity) $\Rightarrow$ \textbf{Measure theory}: The sum rule $x \oplus y = x + y$ for disjoint events.
\item \textbf{Symmetries 3--4} (product distributivity, product associativity) $\Rightarrow$ \textbf{Independence}: The product form for independent measures.
\item \textbf{Symmetry 5} (chaining associativity) $\Rightarrow$ \textbf{Probability calculus}: Conditional probability and Bayes' theorem.
\end{enumerate}

Each symmetry becomes an \emph{axiom of quantification}, and these axioms uniquely determine the calculus.
K\&S emphasize:
\begin{itemize}
\item \textbf{Minimal assumptions}: No continuity, differentiability, or negation required.
\item \textbf{Constructive proofs}: Build from finite systems, avoiding unobservable infinitary subtleties.
\item \textbf{Wide applicability}: Foundations cover measure theory, probability, divergence, and entropy.
\end{itemize}

\subsection{This Formalization}

We formalize K\&S Appendices A and B in Lean 4, providing:
\begin{itemize}
\item Precise statement of axioms and symmetries
\item \textbf{Three independent proof paths} for the representation theorem:
  \begin{enumerate}
  \item Grid-based induction (following K\&S's paper structure)
  \item Dedekind cuts (classical Hölder-style construction)
  \item H\"older embedding via Eric Paul's OrderedSemigroups library
  \end{enumerate}
\item Explicit treatment of implicit assumptions (linear order, separation property)
\item Connection to the classical \textbf{NoAnomalousPairs} condition (Alimov 1950)
\item Non-Boolean examples demonstrating genuine generality
\item Extensions to handle independence and products
\item Counterexamples showing necessity of assumptions
\end{itemize}

The formalization reveals that K\&S's informal proofs are essentially correct, but rely on several implicit assumptions that become apparent only when formalizing.
These details are documented in Appendix~\ref{app:corrections}.

A key finding of this formalization is that the \textbf{NoAnomalousPairs} condition from classical ordered semigroup theory (Alimov 1950, Fuchs 1963) is sufficient for the representation theorem.
This connects K\&S's work to the century-old H\"older embedding theorem for ordered groups.

\section{The Knuth-Skilling Axioms}
\label{sec:axioms}

\subsection{Informal Statement}

K\&S model the world as being in one of finitely many mutually exclusive states.
States correspond to \emph{atoms}, combined via logical OR to form elements of a Boolean lattice.
A \emph{valuation} assigns real numbers to lattice elements, faithful to the lattice order.

For disjoint elements $\mathtt{x}, \mathtt{y}, \mathtt{z}$, K\&S require:
\begin{itemize}
\item \textbf{Axiom 0} (Fidelity): $x > 0$ (positive valuations)
\item \textbf{Axiom 1} (Monotonicity): $x < y \Rightarrow x \oplus z < y \oplus z$ and $z \oplus x < z \oplus y$
\item \textbf{Axiom 2} (Associativity): $(x \oplus y) \oplus z = x \oplus (y \oplus z)$
\end{itemize}

These three axioms suffice to derive the sum rule $x \oplus y = x + y$ (Appendix A).

For independent measures on product spaces:
\begin{itemize}
\item \textbf{Axiom 3} (Product Distributivity): $(x \otimes t) \oplus (y \otimes t) = (x \oplus y) \otimes t$
\item \textbf{Axiom 4} (Product Associativity): $(u \otimes v) \otimes w = u \otimes (v \otimes w)$
\end{itemize}

These determine the multiplicative form of $\otimes$ (Appendix B).

\subsection{Formal Definition}

We formalize the basic K\&S algebra structure as a linearly ordered monoid with strict monotonicity:

\begin{lstlisting}[caption={Core K\&S algebra axioms}]
class KnuthSkillingAlgebra (α : Type*) extends LinearOrder α where
  op : α → α → α                                    -- ⊕ combination
  ident : α                                         -- identity element
  op_assoc : ∀ x y z, op (op x y) z = op x (op y z)  -- Axiom 2
  op_ident_right : ∀ x, op x ident = x
  op_ident_left : ∀ x, op ident x = x
  op_strictMono_left : ∀ y, StrictMono (fun x => op x y)   -- Axiom 1a
  op_strictMono_right : ∀ x, StrictMono (fun y => op x y)  -- Axiom 1b
  ident_le : ∀ x, ident ≤ x                        -- Axiom 0
\end{lstlisting}

\textbf{Note:} K\&S never explicitly state that elements are totally ordered, but their proofs rely on trichotomy ($x < y$, $x = y$, or $x > y$).
Our formalization makes this explicit via \texttt{LinearOrder}.

\subsection{Iteration Notation}

Following K\&S, we define iterated application of the operation:

\begin{lstlisting}[caption={Iteration definition}]
def iterate_op (x : α) : ℕ → α
  | 0 => ident
  | n + 1 => op x (iterate_op x n)
\end{lstlisting}

This builds the sequence $\text{ident}, x, x \oplus x, x \oplus (x \oplus x), \ldots$, which K\&S write as ``$0$ of $x$'', ``$1$ of $x$'', ``$2$ of $x$'', etc.

\section{The Separation Property}
\label{sec:separation}

\subsection{Definition}

K\&S's Appendix A proof requires finding rational approximations to any pair of distinct values.
This is implicit in their ``induction to more than one type of atom'' construction.
Following a suggestion by B.\ Goertzel, we isolate this as an explicit axiom---the \emph{separation property}:

\begin{lstlisting}[caption={Separation axiom}]
class KSSeparation (α : Type*) [KnuthSkillingAlgebra α] where
  separation : ∀ {a x y : α}, ident < a → ident < x → ident < y → x < y →
    ∃ n m : ℕ, 0 < m ∧
      iterate_op x m < iterate_op a n ∧
      iterate_op a n ≤ iterate_op y m
\end{lstlisting}

\textbf{Intuition:} For any base $a > \text{ident}$ and distinct $x < y$, we can find exponents $(n, m)$ such that $x^m < a^n \leq y^m$.
This lets us ``separate'' $x$ and $y$ using rational powers of $a$.

\subsection{Necessity}

The separation property is \emph{necessary} for representation.
Counterexample: Consider $\mathbb{N} \times \mathbb{N}$ with lexicographic order:
\[(n_1, m_1) < (n_2, m_2) \iff n_1 < n_2 \text{ or } (n_1 = n_2 \text{ and } m_1 < m_2)\]
and operation:
\[(n_1, m_1) \oplus (n_2, m_2) = (n_1 + n_2, m_1 + m_2)\]

This satisfies Axioms 0--2 but \emph{not} separation: $(1, 1) < (2, 0)$, but no powers of $(1, 0)$ can separate them.
Consequently, no order embedding into $\mathbb{R}$ exists.

\subsection{Derivable Consequences}

From separation, we derive:
\begin{itemize}
\item \textbf{Archimedean property}: For any $x > \text{ident}$ and $y$, there exists $n$ such that $\text{iterate\_op}\, x\, n > y$.
\item \textbf{Commutativity}: $\text{op}\, x\, y = \text{op}\, y\, x$ for all $x, y$.
\end{itemize}

These are proven in \texttt{Additive/Axioms/SandwichSeparation.lean} and \texttt{Additive/Proofs/GridInduction/Core/SeparationImpliesCommutative.lean}.

\section{The Representation Theorem (Appendix A)}
\label{sec:representation}

\subsection{Statement}

\begin{theorem}[K\&S Appendix A]
\label{thm:representation}
Let $\alpha$ be a type with \texttt{KnuthSkillingAlgebraBase}.
Under appropriate regularity conditions (see below), there exists an order embedding $\Theta : \alpha \to \mathbb{R}$ such that:
\begin{enumerate}
\item $\Theta(\text{ident}) = 0$
\item $\forall x, y : \alpha,\; \Theta(\text{op}\, x\, y) = \Theta(x) + \Theta(y)$
\end{enumerate}
\end{theorem}

This is K\&S's main result: Axioms 1--2 plus regularity uniquely determine that $\oplus$ corresponds to addition in $\mathbb{R}$ (up to order-preserving regrade).

\subsection{Three Proof Approaches}

Our formalization provides \textbf{three independent proof paths}, each with different assumptions and characteristics:

\subsubsection{Approach 1: Grid-Based Induction (K\&S Style)}

\textbf{Assumptions}: \texttt{KSSeparation} + \texttt{RepresentationGlobalization}

\textbf{Technique}: K\&S's constructive proof proceeds by:
\begin{enumerate}
\item \textbf{One type}: For a single atom $a$, assign $\text{iterate\_op}\, a\, n \mapsto n \cdot \Theta(a)$.
\item \textbf{Induction}: For a new atom $d$, use separation to find $(u, v)$ such that $a^u < d^v \leq a^{u+1}$.
   Define $\Theta(d) = (u + \delta) / v$ for appropriate $\delta \in [0, 1]$.
\item \textbf{Globalization}: Show this extends to all combinations consistently (the ``triple family trick'').
\end{enumerate}

\textbf{Files}: \texttt{Additive/Proofs/GridInduction/Core/Induction/ThetaPrime.lean} (3,613 lines), \\
\hspace*{2em} \texttt{Additive/Proofs/GridInduction/Core/MultiGrid.lean} (2,999 lines)

\textbf{Pros}: Follows K\&S paper closely; self-contained \\
\textbf{Cons}: Complex grid management; hard to maintain

\begin{lstlisting}[caption={Grid-based representation theorem}]
theorem associativity_representation
    (α : Type*) [KnuthSkillingAlgebra α] [KSSeparation α]
    [RepresentationGlobalization α] :
    ∃ Θ : α → ℝ,
      (∀ a b : α, a ≤ b ↔ Θ a ≤ Θ b) ∧
      Θ ident = 0 ∧
      ∀ x y : α, Θ (op x y) = Θ x + Θ y
\end{lstlisting}

\subsubsection{Approach 2: Dedekind Cuts (Classical Style)}

\textbf{Assumptions}: \texttt{KSSeparation} + \texttt{KSSeparationStrict}

\textbf{Technique}: Construct $\Theta$ directly via Dedekind cuts:
\begin{enumerate}
\item For a base element $a > \text{ident}$, define the \emph{cut set}:
  \[\text{cutSet}(a, x) = \{q \in \mathbb{Q} : q \geq 0 \land \exists m,n.\; a^m \leq x^n \land q = m/n\}\]
\item Define $\Theta(x) = \sup(\text{cutSet}(a, x))$
\item Prove this is well-defined, order-preserving, and additive
\end{enumerate}

\textbf{Files}: \texttt{Additive/Proofs/DirectCuts/DirectCuts.lean}

\textbf{Pros}: Classical approach; more direct than grid method \\
\textbf{Cons}: Requires strict separation; still substantial code

\begin{lstlisting}[caption={Cuts-based representation theorem}]
theorem associativity_representation_cuts
    (α : Type*) [KnuthSkillingAlgebra α] [KSSeparation α]
    [KSSeparationStrict α] :
    ∃ Θ : α → ℝ,
      (∀ a b : α, a ≤ b ↔ Θ a ≤ Θ b) ∧
      Θ ident = 0 ∧
      ∀ x y : α, Θ (op x y) = Θ x + Θ y
\end{lstlisting}

\subsubsection{Approach 3: H\"older Embedding (Classical Route)}

\textbf{Assumptions}: \texttt{NoAnomalousPairs} only!

\textbf{Technique}: Leverage Eric Paul's formalized H\"older theorem for ordered semigroups:
\begin{enumerate}
\item Show \texttt{KnuthSkillingAlgebraBase} provides an \texttt{IsOrderedCancelSemigroup} instance
\item Translate our \texttt{AnomalousPair} definition to Eric's \texttt{anomalous\_pair}
\item Apply Eric's \texttt{holder\_not\_anom}: $\neg\text{has\_anomalous\_pair} \Rightarrow \alpha \hookrightarrow (\mathbb{R}, +)$
\item Extract $\Theta$ from the multiplicative order isomorphism
\end{enumerate}

\textbf{Files}: \texttt{Additive/Proofs/OrderedSemigroupEmbedding/HolderEmbedding.lean} (761 lines) + Eric's library

\textbf{Pros}: Classical assumptions; shortest local code; most elegant \\
\textbf{Cons}: Depends on external library

\begin{lstlisting}[caption={H\"older-based representation theorem (classical route)}]
theorem representation_from_noAnomalousPairs
    (α : Type*) [KnuthSkillingAlgebraBase α] [NoAnomalousPairs α] :
    ∃ Θ : α → ℝ,
      (∀ a b : α, a ≤ b ↔ Θ a ≤ Θ b) ∧
      Θ ident = 0 ∧
      ∀ x y : α, Θ (op x y) = Θ x + Θ y
\end{lstlisting}

\subsection{The NoAnomalousPairs Condition}

\begin{definition}[Anomalous Pair]
Elements $a, b$ form an \emph{anomalous pair} if for every $n > 0$:
\[(a^n < b^n < a^{n+1}) \quad\text{or}\quad (a^n > b^n > a^{n+1})\]
i.e., the iterates stay ``squeezed'' forever.
\end{definition}

\begin{definition}[NoAnomalousPairs]
A K\&S algebra has \emph{no anomalous pairs} if no such pair exists:
\[\forall a\, b,\; \neg\text{AnomalousPair}(a, b)\]
\end{definition}

This condition is from classical ordered semigroup theory (Alimov 1950, Fuchs 1963).
It is \textbf{weaker} than \texttt{KSSeparation}:

\begin{lstlisting}[caption={KSSeparation implies NoAnomalousPairs (under IdentIsMinimum)}]
theorem noAnomalousPairs_of_KSSeparation_with_IdentMin
    [KSSeparation α] [IdentIsMinimum α] : NoAnomalousPairs α
\end{lstlisting}

\textbf{Note}: \texttt{IdentIsMinimum} ($\forall a,\; \text{ident} \leq a$) is natural for the K\&S probability setting where \texttt{ident} represents the certain event.

\subsection{Comparison of Approaches}

\begin{center}
\begin{tabular}{lccc}
\textbf{Metric} & \textbf{Grid} & \textbf{Cuts} & \textbf{H\"older} \\
\hline
Local lines of code & $\sim$4000 & $\sim$1100 & $\sim$420 \\
Total lines (incl.\ deps) & $\sim$4000 & $\sim$1100 & $\sim$1900 \\
Assumptions & Medium & Medium+ & \textbf{Classical} \\
Self-contained & \checkmark & \checkmark & -- \\
Follows K\&S paper & \checkmark & partial & -- \\
Conceptual clarity & Low & Medium & \textbf{High} \\
Maintainability & Low & Medium & \textbf{High} \\
\end{tabular}
\end{center}

\textbf{Recommendation}: For practical use, the H\"older approach is preferred due to its minimal assumptions and elegant proof structure.
For pedagogical purposes or K\&S paper alignment, the cuts approach provides a good balance.

\subsection{Unified Interface}

All three approaches provide instances of a common interface:

\begin{lstlisting}[caption={Unified representation theorem interface}]
class HasRepresentationTheorem (α : Type*) [KnuthSkillingAlgebraBase α] : Prop where
  exists_representation :
    ∃ Θ : α → ℝ,
      (∀ a b : α, a ≤ b ↔ Θ a ≤ Θ b) ∧
      Θ ident = 0 ∧
      ∀ x y : α, Θ (op x y) = Θ x + Θ y

-- Instances from each approach:
instance holder_hasRepresentationTheorem [NoAnomalousPairs α]
instance grid_hasRepresentationTheorem [KSSeparation α] [RepresentationGlobalization α]
instance cuts_hasRepresentationTheorem [KSSeparation α] [KSSeparationStrict α]
\end{lstlisting}

\textbf{File}: \texttt{Additive/Proofs/GridInduction/Comparison.lean}

\section{The Product Theorem (Appendix B)}
\label{sec:product}

\textbf{Lean status: COMPLETE (no \texttt{sorry}, no \texttt{axiom}).}

After Appendix A regrades $\oplus$ to real addition, K\&S introduce a second scalar operation
$\otimes$ for the direct product of independent lattices.  Appendix B shows that $\otimes$ must
be multiplication up to a single global scale constant.

In Lean we formalize this conclusion in three complementary ways (all sorry-free):
\begin{enumerate}
\item \textbf{Product-equation route} (closest to K\&S's presentation):
  assume an additive order-isomorphism representation of $\otimes$ on $\mathbb{R}_{>0}$,
  i.e. $\Theta(x \otimes y) = \Theta x + \Theta y$, together with distributivity over $+$ (Axiom~3).
  This yields the Appendix~B product equation for $\Psi := \Theta^{-1}$, which we solve to obtain
  $\Psi(x) = C e^{Ax}$ and hence $(x \otimes y) = (xy)/C$.
  (Lean: \texttt{Multiplicative/Main.lean}.)
\item \textbf{Lean-friendly "avoid Appendix A again" route}:
  do \emph{not} assume a representation theorem for $\otimes$.
  Instead, package the regularity properties as \texttt{TensorRegularity} (associativity + injectivity of $t \mapsto 1 \otimes t$)
  and combine with distributivity (Axiom~3) to derive $(x \otimes y) = (xy)/C$ directly.
  (Lean: \texttt{Multiplicative/AczelTheorem.lean}.)
\item \textbf{WARNING: Circular Reasoning Anti-Pattern}.
  A previous file \texttt{EventBridge.lean} was \textbf{deleted} because it contained circular reasoning:
  it defined \texttt{mulTensor := multiplication} and then ``proved'' multiplication equals scaled multiplication.
  \textbf{This proves nothing!}  The correct approach (what \texttt{AczelTheorem.lean} does) is to start with
  an \emph{abstract} tensor $\otimes$ satisfying Axioms 3--4 and prove that ANY such tensor equals scaled multiplication.
\end{enumerate}

\textbf{Key point:} the Appendix~A axiom \texttt{ident\_le} (identity is the bottom element) is
appropriate for $\oplus$ but structurally incompatible with $\otimes$ on $(0,\infty)$, where the
identity is $1$ and is \emph{not} a minimum element.  Our Appendix~B development avoids this
mismatch by never trying to model $\otimes$ as a "bottomed" additive monoid.

\textbf{Axiom audit:} We checked all \texttt{Multiplicative/*.lean} files for \texttt{axiom}, \texttt{sorry}, or \texttt{admit}---none found.
The injectivity requirement in \texttt{TensorRegularity} is \emph{derived} from distributivity + commutativity
via \texttt{TensorRegularity.of\_assoc\_of\_distrib\_of\_comm}, not assumed.

\subsection{K\&S's Approach: Fibonacci Recurrence}

For independent random variables, K\&S derive that the measure function $\Psi$ must satisfy:
\[\Psi(\tau + \xi) + \Psi(\tau + \eta) = \Psi(\tau + \zeta(\xi, \eta))\]
where $\tau, \xi, \eta$ are independent real variables.

K\&S prove the unique solution $\Psi(x) = C \cdot e^{Ax}$ via:
\begin{enumerate}
\item \textbf{2-term recurrence} (special case $\xi = \eta$): Shows $\Psi(\theta + na) = 2^n \Psi(\theta)$
\item \textbf{3-term recurrence} (Fibonacci-like): Produces golden ratio behavior
\item \textbf{Density argument}: Since $b/a$ is irrational, offsets $mb - na$ approximate any real
\end{enumerate}

This clever proof avoids continuity assumptions and works purely combinatorially.

\subsection{Formalization Notes}

\begin{itemize}
\item \textbf{K\&S's Fibonacci proof}: COMPLETE.  The full Fibonacci recurrence argument is formalized
  in \texttt{Multiplicative/Proofs/Direct/FibonacciProof.lean}, including:
  \begin{enumerate}
  \item Golden ratio irrationality (\texttt{goldenRatio\_irrational})
  \item Dense offsets $\{m - n\phi : m,n \in \mathbb{N}, n \geq 1\}$ in $\mathbb{R}$ (\texttt{dense\_fib\_linear\_combinations})
  \item The 2-term and 3-term recurrences that produce Fibonacci behavior
  \end{enumerate}

\item \textbf{KEY ACHIEVEMENT: Continuity derivation}.  We prove that continuity follows from
  the ProductEquation + strict monotonicity + positivity, avoiding the need to assume continuity:
  \begin{lstlisting}[caption={Deriving continuity from ProductEquation}]
theorem productEquation_strictMono_pos_continuous
    {Ψ : ℝ → ℝ} {ζ : ℝ → ℝ → ℝ}
    (hProd : ProductEquation Ψ ζ)
    (hPos : ∀ x, 0 < Ψ x)
    (hMono : StrictMono Ψ) : Continuous Ψ
  \end{lstlisting}
  \textbf{Proof idea}: ProductEquation implies $\Psi(\theta + na) = 2^n \Psi(\theta)$ (doubling).
  Combined with the dense range $\{m \cdot C/2^k : m \geq 1, k \in \mathbb{N}\}$ where $C = \Psi(0)$,
  strict monotonicity forces $\Psi$ to be continuous (no jumps possible).

\item \textbf{Main theorem} (K\&S Appendix B):
  \begin{lstlisting}[caption={K\&S Appendix B: StrictMono case}]
theorem ks_appendix_b_fibonacci_strictMono
    (Ψ : ℝ → ℝ) (ζ : ℝ → ℝ → ℝ)
    (hProd : ProductEquation Ψ ζ)
    (hPos : ∀ x, 0 < Ψ x)
    (hMono : StrictMono Ψ) :
    ∃ C A : ℝ, 0 < C ∧ ∀ x : ℝ, Ψ x = C * Real.exp (A * x)
  \end{lstlisting}
  This theorem is \textbf{sorry-free} and combines the continuity derivation with
  \texttt{productEquation\_solution\_of\_continuous\_strictMono}.

\item \textbf{Mathlib support}: we use \texttt{Real.log\_mul} and related properties of $\exp/\log$ for the
  log-coordinate arguments.

\item \textbf{Archived alternatives}: Two incomplete approaches (deriving StrictMono from Continuous,
  and deriving StrictMono from weak Monotone) are preserved in \texttt{Multiplicative/Scratch/FibonacciProofScratch.lean}
  for reference.  These were not needed because K\&S Appendix A already provides StrictMono from
  the order isomorphism representation.
\end{itemize}

\section{The Variational Theorem (Appendix C)}
\label{sec:variational}

K\&S's Appendix C addresses entropy and divergence via a \emph{variational approach}.
This section contains a subtle but important error in the original paper that our formalization corrects.

\subsection{The Variational Functional Equation}

K\&S consider a potential function $H(m)$ satisfying the functional equation:
\begin{equation}
H'(m_x \cdot m_y) = \lambda(m_x) + \mu(m_y) \label{eq:variational}
\end{equation}
where $H' = dH/dm$ and $m_x, m_y > 0$.

\subsection{Path B: Lagrange Multipliers (Deriving the Functional Equation)}

In the paper, Equation~\eqref{eq:variational} is motivated by a Lagrange multiplier argument:
K\&S begin with the variational constraint form (their Eq.~\texttt{vary2}), introduce multipliers
(\texttt{vary3}), and then specialize to a two-dimensional $x$-by-$y$ direct-product application.
There, the direct-product rule gives a target value $m(x\times y)=m_x m_y$, and taking the
coordinate derivative with respect to $m(x\times y)$ yields their Eq.~\texttt{Hproduct}:
\[
H'_{xy}(m_x m_y) = \lambda_1 f_1(m_x) + \lambda_2 f_2(m_y).
\]

In Lean we formalize the \emph{coordinate-derivative calculation} explicitly, without smuggling it in
as an axiom.  The key lemma is
\texttt{lagrange\_coordinate\_deriv\_eq\_product}
(\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Variational/Main.lean}), which models
``$\partial/\partial\,m(x\times y)$'' as the 1D derivative along the update map
$t \mapsto \mathrm{update}\ m\ (x,y)\ t$.
It assumes only local differentiability of the relevant terms and a stationarity hypothesis stated as a
\texttt{HasDerivAt ... 0 ...} condition (so this is a real derivative, not the default \texttt{deriv} value at non-differentiable points).

\begin{lstlisting}[caption={Path B: coordinate-derivative separation at a stationary point}]
theorem lagrange_coordinate_deriv_eq_product
    (H : Real -> Real) (g : X -> Real -> Real) (h : Y -> Real -> Real)
    (alpha : X -> Real) (beta : Y -> Real) (m : Prod X Y -> Real) (x0 : X) (y0 : Y)
    (hprod : m (x0, y0) = rowSum m x0 * colSum m y0)
    (hcrit : HasDerivAt (fun t => productLagrangian H g h alpha beta
              (Function.update m (x0, y0) t)) 0 (m (x0, y0))) :
    deriv H (rowSum m x0 * colSum m y0)
      = alpha x0 * deriv (g x0) (rowSum m x0)
        + beta y0 * deriv (h y0) (colSum m y0)
\end{lstlisting}

This establishes K\&S's separated form \emph{at a stationary point} of a separable Lagrangian.
The global functional-equation \eqref{eq:variational} is then treated by Path A (below), which is the
classification step that yields the logarithmic solution.

\paragraph{Fully honest stationarity.}
To avoid assuming stationarity as an axiom, we also provide a variant that derives the
\texttt{HasDerivAt ... 0 ...} hypothesis from a local extremum via Fermat's theorem:
\texttt{lagrange\_coordinate\_deriv\_eq\_product\_of\_isLocalExtr}
\ (\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Variational/Main.lean}).
We then package the result in the exact ``K\&S \texttt{Hproduct}'' shape as
\texttt{lagrange\_Hproduct\_eq\_of\_isLocalExtr}.

\paragraph{Universality/richness (Path B $\to$ Path A).}
Path B establishes a separated coordinate equation at a \emph{particular} stationary point of a
separable constrained problem.
To treat this as a functional equation valid for all positive pairs $(m_x,m_y)$ (the input required
by Path A), K\&S rely on an informal ``generality across applications'' premise: the same potential
is intended to apply uniformly across a sufficiently rich class of inference problems.
In Lean we make this bridge assumption explicit as the structure
\texttt{KSVariationalGenerality} (\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Variational/Main.lean}),
so nothing is smuggled: either you assume it, or you do not get a global functional equation.

\subsection{K\&S's Differentiability Claim (and Why It's Wrong)}

K\&S assert that ``\emph{any variational potential must \ldots be differentiable at least once}'' because it applies to arbitrarily small perturbations.
\textbf{This claim is false.}

\begin{example}[Variational minimum without differentiability]
Consider $H(x) = |x|$ on $\mathbb{R}$.
The point $x_\star = 0$ is a global minimizer: for every small perturbation $h$,
\[H(0 + h) - H(0) = |h| \geq 0.\]
This is exactly the variational inequality ``no small perturbation decreases $H$''.
But $H$ is \emph{not differentiable} at $0$: the one-sided derivatives are $-1$ and $+1$.
\end{example}

\begin{example}[K\&S-shaped example on $(0,\infty)$]
Consider $H(m) = |\log m|$ on $(0, \infty)$.
It has a unique minimum at $m_\star = 1$, but is not differentiable there.
This directly refutes K\&S's claim in their specific modeling domain.
\end{example}

Nonsmooth variational problems are not exotic: absolute value, max functions, and $\ell_1$ penalties are core tools in modern optimization.
Rockafellar--Wets~\cite{RockafellarWets} explicitly stress that max/min operations often fail to preserve smoothness.
These counterexamples are formalized in \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Counterexamples/VariationalNonsmooth.lean}.

\subsection{The Correct Approach: Measurability}

\subsubsection{Minimal assumptions (sound formal statement)}

It is useful to separate two logically distinct issues:
\begin{enumerate}
\item \emph{What hypotheses are needed for the functional-equation \emph{classification} $H'(m)=B+C\log m$?}
\item \emph{What hypotheses are needed to interpret $H'$ as an actual derivative of some potential $H$?}
\end{enumerate}

Our Lean formalization provides both formulations, and makes each assumption explicit.
The minimal package for the \emph{classification step} is:
\begin{description}
\item[\textbf{(C0) Domain / log-coordinates.}] We work on positive masses $m>0$ so that $\log m$ is defined and
  $\log(m_x m_y)=\log m_x+\log m_y$ can be used.
\item[\textbf{(C1) Variational functional equation.}] The separation-of-variables equation
  $H'(m_x m_y)=\lambda(m_x)+\mu(m_y)$ holds for all $m_x,m_y>0$ (Equation~\eqref{eq:variational}).
\item[\textbf{(C2) Anti-pathology regularity.}] Some regularity is required to rule out Hamel-basis pathologies of
  Cauchy's equation after the $u=\log m$ change of variables.  We take \textbf{Borel measurability} of $H'$ as the
  core hypothesis.
\end{description}

This is exactly the statement proved by our main Lean theorem
\texttt{variationalEquation\_solution\_measurable}
(\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Variational/Main.lean}).
It \emph{does not} assume that $H'$ comes from a differentiable potential.

If one also wants a theorem phrased in terms of an underlying potential $H$, one adds an optional hypothesis:
\begin{description}
\item[\textbf{(C3) Derivative interpretation (optional).}] Either set $H'=\mathrm{deriv}\,H$, or assume
  $\mathrm{HasDerivAt}\,H\,(H'(m))\,m$ for all $m>0$.
\end{description}
In Lean, (C3) is captured by the corollary \texttt{variationalEquation\_solution} (the \texttt{deriv} version), which
automatically discharges (C2) via \texttt{measurable\_deriv}.

Our formalization makes the regularity assumption \emph{explicit}.
The key insight is that the functional equation~\eqref{eq:variational}, after the log-coordinate change $u = \log m$, reduces to \textbf{Cauchy's additive functional equation}:
\[f(u + v) = f(u) + f(v)\]

Without regularity, Cauchy's equation has pathological non-linear solutions (via Hamel basis constructions, assuming the Axiom of Choice).
To obtain the unique linear solution $f(t) = Ct$, \emph{some} regularity is required.
The existence of non-linear additive solutions is shown by a standard Hamel-basis construction; we formalize this as a counterexample generator in \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Counterexamples/CauchyPathology.lean}.

The standard hierarchy of sufficient conditions includes:
\begin{itemize}
\item Measurable (weakest commonly used)
\item Continuous at one point
\item Bounded on any interval
\item Monotone on any interval
\end{itemize}

\begin{center}
\begin{tabular}{|l|c|c|p{6.2cm}|}
\hline
\textbf{Regularity gate for Cauchy} & \textbf{Forces linearity?} & \textbf{Automatic for $\mathrm{deriv}$?} & \textbf{Comment} \\
\hline
Borel measurable & yes & yes & Our chosen explicit replacement for K\&S ``blurring''; discharged by \texttt{measurable\_deriv}. \\
\hline
Continuous at one point & yes & no & Classical Shannon/Faddeev-style hypothesis; also implies measurability. \\
\hline
Monotone on an interval & yes & no & Order-theoretic alternative; implies measurability (hence works with our proof). \\
\hline
Bounded on an interval & yes & no & Another standard gate excluding Hamel-basis pathologies. \\
\hline
\end{tabular}
\end{center}

\textbf{Our choice}: We use \textbf{Borel measurability} as the regularity assumption because:
\begin{enumerate}
\item It is among the weakest standard conditions
\item It is \textbf{automatic} for derivatives: if $H$ is differentiable, then $H' = \text{deriv}\, H$ is Borel measurable (via \texttt{measurable\_deriv} in mathlib)
\item It matches what K\&S's convolution ``blurring'' argument implicitly requires (one cannot even write the convolution integral without measurability/integrability hypotheses)
\end{enumerate}

\begin{lstlisting}[caption={Main Variational Theorem (measurable version)}]
theorem variationalEquation_solution_measurable
    (H' : ℝ → ℝ) (lam mu : ℝ → ℝ)
    (hMeas : Measurable H')
    (hV : VariationalEquation H' lam mu) :
    ∃ B C : ℝ, ∀ m : ℝ, 0 < m →
      H' m = B + C * log m
\end{lstlisting}

\begin{lstlisting}[caption={Corollary for actual derivatives (regularity automatic)}]
theorem variationalEquation_solution
    (H : ℝ → ℝ) (lam mu : ℝ → ℝ)
    (hV : VariationalEquation (deriv H) lam mu) :
    ∃ B C : ℝ, ∀ m : ℝ, 0 < m →
      deriv H m = B + C * log m
\end{lstlisting}

\subsection{Alternative: Monotonicity (K\&S-Spirited)}

An alternative regularity assumption that mirrors Appendix A's order-theoretic style is \textbf{monotonicity}:

\begin{itemize}
\item Monotone $\Rightarrow$ Measurable $\Rightarrow$ log form
\item Monotonicity is physically motivated: entropy has decreasing marginal value
\item This parallels Appendix A where order-preservation is central
\end{itemize}

However, monotonicity is \emph{stronger} than measurability (not all measurable functions are monotone).
Our core theorem uses the weaker assumption; monotonicity can be stated as a corollary.

\subsection{Shore--Johnson (1980) comparison}

Shore--Johnson's axioms for maximum entropy inference (IEEE Trans.\ IT, 1980) include a ``system independence''
requirement that can be viewed as another route to the same logarithmic structure.
In a discrete, summand-level setting:
\begin{itemize}
\item Assume an atomic divergence $d(p,q)$ satisfies $d(0,q)=0$ and that the sum
  $\sum_i d(p_i,q_i)$ is additive over product distributions (an SJ4-style hypothesis).
\item Dirac-delta test distributions then force a multiplicative Cauchy equation for
  $g(q) := d(1,q)$ on the probability domain $0<q\le 1$.
\item With a regularity gate (we again take measurability), this implies $g(q)=C\log q$.
\end{itemize}

We formalize this ``Dirac extraction $\to$ log'' lemma as
\texttt{d\_one\_eq\_const\_mul\_log\_of\_measurable}
in \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/ShoreJohnson.lean}.
This provides a clean comparison point between Shore--Johnson's system-independence intuition and
K\&S Appendix~C: both rely on the same functional-equation rigidity, and both require an explicit
anti-pathology hypothesis to exclude Hamel-basis solutions.

We also formalize a small but useful \emph{KL uniqueness} lemma (``uniqueness up to scale, within a
ratio-form class''):
\begin{itemize}
\item If $g:(0,\infty)\to\mathbb{R}$ satisfies $g(xy)=g(x)+g(y)$ and is Borel measurable, then $g(x)=C\log x$.
\item Hence any ratio-form atom divergence $d(w,u)=w\,g(w/u)$ (for $w,u>0$) must be a constant multiple of the KL atom
$w\log(w/u)$.
\item Without the regularity gate, uniqueness fails: we exhibit a multiplicative-additive (Hamel-basis) counterexample
and the induced ratio-form $d$ is \emph{not} a constant multiple of KL.
\end{itemize}
These results are proved in \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/ShoreJohnson/KL.lean}.
For convenience, the project re-exports the key Shore--Johnson lemmas we use in
\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/ShoreJohnson/Theorem.lean}.
We then connect the resulting atom-level KL identity to the project's finite KL divergence
definition on \texttt{ProbDist} (Section~8 layer) in
\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/ShoreJohnson/Bridge.lean}.

\subsection{The Entropy/Divergence Form}

From the variational theorem, integrating $H'(m) = B + C \log m$ yields:
\[H(m) = A + Bm + C(m \log m - m)\]

With appropriate choices of constants ($C = 1$, $B = -\log u$, $A = u$), this specializes to the \textbf{Kullback--Leibler divergence}:
\[D(w \| u) = \sum_i \left( u_i - w_i + w_i \log \frac{w_i}{u_i} \right)\]

This is formalized in \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Information/Divergence.lean} with proofs of non-negativity, zero iff equal, and asymmetry.
For probability distributions (equal total mass), the linear terms $\sum_i (u_i - w_i)$ cancel, leaving the usual $\sum_i w_i \log(w_i/u_i)$.

\subsection{Summary: What K\&S Should Have Said}

K\&S informally assert that a variational potential must be $C^1$ because it applies to arbitrarily small perturbations.
This is not a logical consequence of local optimality.
Standard nonsmooth variational analysis replaces gradients with subgradients (Rockafellar--Wets; Clarke).

In our formalization, we \textbf{state measurability explicitly} as an analytic assumption.
This is also exactly what is needed to rule out pathological Cauchy solutions and obtain the unique logarithmic form.
No circularity: we assume (functional equation + measurability), and \emph{derive} the log form.

\section{Probability and Conditional Inference}

K\&S's Axiom 5 (chaining associativity) extends the framework to conditional probability.
We formalize the chain product rule, Bayes' theorem, and the ratio-of-measures representation in
\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Probability/ConditionalProbability/Basic.lean}.

\section{Information and Entropy (Section 8)}

K\&S Section 8 specializes the variational potential to (i) information/divergence for probability
distributions and (ii) Shannon entropy.
We formalize this in
\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Information/InformationEntropy.lean}.

For comparison (and to honor the historical development), we also formalize classical axiomatic
characterizations of Shannon entropy \emph{independently of K\&S}:
\begin{itemize}
\item Shannon's original 1948 axioms + uniqueness (Appendix~2 of Shannon): \texttt{Mettapedia/InformationTheory/ShannonEntropy/Shannon1948.lean} (sorry-free).
\item Faddeev (1956) and Shannon--Khinchin (1957) axiom systems: \texttt{Mettapedia/InformationTheory/ShannonEntropy/Faddeev.lean} and \texttt{Mettapedia/InformationTheory/ShannonEntropy/ShannonKhinchin.lean} (both WIP).
\item The equivalence \emph{glue} is kept in a separate file so the Shannon--Khinchin development does not depend on Faddeev: \texttt{Mettapedia/InformationTheory/ShannonEntropy/Equivalence.lean}.
\end{itemize}

Highlights:
\begin{itemize}
\item \textbf{Finite and extended KL}: a finite KL definition under ``support positivity'' and an extended
  \texttt{ENNReal}-valued version returning $\top$ when $p_i>0$ but $q_i=0$
  (\texttt{klDivergence}, \texttt{klDivergenceTop}).
\item \textbf{Mathlib KL bridge (finite $\to$ general)}: we show our \texttt{klDivergenceTop} on \texttt{Fin n}
  agrees with mathlib's measure-theoretic \texttt{InformationTheory.klDiv} on the associated Dirac-sum measures
  (\texttt{klDiv\_toMeasureTop\_eq\_klDivergenceTop} in
  \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Information/InformationEntropyMathlib.lean}).
  This provides a clean ``infinite/continuum'' variant by reusing mathlib's general KL divergence.
\item \textbf{Mathlib KL bridge (countable discrete)}: for sequences on $\mathbb{N}$ we connect the K\&S
  countable divergence sum \texttt{divergenceInf} to \texttt{InformationTheory.klDiv} for Dirac-sum measures
  (\texttt{Seq.klDiv\_toMeasureTop\_eq\_divergenceInfTop} and
  \texttt{Seq.klDiv\_toMeasureTop\_eq\_ofReal\_divergenceInf} in
  \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Information/DivergenceMathlib.lean}).
\item \textbf{Shannon properties}: entropy of the uniform distribution is $\log n$
  (\texttt{shannonEntropy\_uniform}), strict monotonicity in $n$ for uniform distributions
  (\texttt{shannonEntropy\_uniform\_strictMono}), grouping/additivity in a binary refinement
  (\texttt{shannonEntropy\_grouping\_probDist}), and continuity on the interior of the simplex
  (\texttt{shannonEntropy\_continuous\_on\_interior}).
\end{itemize}

\section{Conclusion}

The K\&S program successfully derives probability calculus from symmetries.
Our formalization confirms their approach is sound, while making explicit:
\begin{itemize}
\item Linear order (implicit in their trichotomy arguments)
\item Separation property (implicit in rational approximation)
\item Archimedean property (derivable from separation)
\item Commutativity (derivable from NoAnomalousPairs)
\end{itemize}

\textbf{Key contributions of this formalization:}

\begin{enumerate}
\item \textbf{Three independent proof paths}: Grid-based (K\&S style), Dedekind cuts (classical), and H\"older embedding (via Eric Paul's library).

\item \textbf{Discovery of minimal assumptions}: The \texttt{NoAnomalousPairs} condition from classical ordered semigroup theory (Alimov 1950) is \emph{sufficient} for the representation theorem.

\item \textbf{Connection to classical mathematics}: K\&S's work fits within the century-old tradition of H\"older embedding theorems for ordered algebraic structures.
Eric Paul's Lean 4 formalization of this classical theory provides a clean, minimal proof path.

\item \textbf{Unified interface}: All three proof approaches provide instances of a common \texttt{HasRepresentationTheorem} typeclass, enabling modular use.
\end{enumerate}

\textbf{Practical recommendation}: For new formalizations building on K\&S, the H\"older approach is preferred:
\begin{itemize}
\item Uses classical assumptions (\texttt{NoAnomalousPairs} only)
\item Shortest local codebase (761 lines vs.\ 6,612 for grid approach)
\item Most mathematically elegant
\item Leverages Eric Paul's battle-tested library
\end{itemize}

The formalized representation theorem provides a constructive, computationally meaningful foundation for inference, now available via multiple independent proof paths.

\appendix

\section{Corrections to the K\&S Axioms}
\label{app:corrections}

Formalization revealed several implicit assumptions and derivable properties in K\&S.
We document these findings with a generous reading of their intent.

\subsection{The Implicit Linear Order}

K\&S never explicitly state that elements are totally ordered, yet their proofs rely on trichotomy: for any $x, y$, exactly one of $x < y$, $x = y$, or $x > y$ holds.

\textbf{Evidence:} K\&S line 1339:
\begin{quote}
``By interchanging $x$ and $y$ in axiom 1, the same relationship holds when `$<$' is replaced throughout by `$>$', and replacement by `$=$' holds trivially. So, in effect, the axiom makes a three-fold assertion''
\end{quote}

This implicitly assumes trichotomy, which we make explicit via \texttt{LinearOrder}.

\subsection{Commutativity: Derivable, But Only With Separation}

K\&S state (Section 3.1):
\begin{quote}
``We also confirm that commutativity is not a necessary assumption. Rather, commutativity of measure is imposed by the associativity and order required of a scalar representation.''
\end{quote}

\textbf{Our finding:} Commutativity is \emph{not} derivable from Axioms 0--2 alone.
It requires the \emph{separation property}.

\textbf{Proof:} Theorem in \texttt{Additive/Proofs/GridInduction/Core/SeparationImpliesCommutative.lean}:
\begin{lstlisting}
theorem op_comm_of_associativity
    (α : Type*) [KnuthSkillingAlgebra α] [KSSeparation α] :
    ∀ x y : α, op x y = op y x
\end{lstlisting}

\textbf{Generous reading:} K\&S likely intended ``associativity and order'' to include the implicit separation property that enables their rational approximation construction.

\subsection{Archimedean: Derivable from Separation}

K\&S's proofs implicitly use the Archimedean property: for any $a > \text{ident}$ and $x$, there exists $n$ such that $a^n > x$.

\textbf{Our finding:} The Archimedean property is \emph{derivable} from separation, not an independent axiom.

\textbf{Proof:} Theorem in \texttt{Additive/Axioms/SandwichSeparation.lean}:
\begin{lstlisting}
theorem op_archimedean_of_separation
    [KSSeparation α] (a x : α) (ha : ident < a) :
    ∃ n : ℕ, x < iterate_op a n
\end{lstlisting}

This is formalization-revealed structure, not a correction to K\&S.

\subsection{The Separation Property Itself}

K\&S's Appendix A construction implicitly assumes we can ``separate'' any two distinct values using rational powers of a base.
We make this explicit as \texttt{KSSeparation}.

\textbf{Evidence:} K\&S's ``induction to more than one type of atom'' (Appendix A, starting line 1437) constructs rationals $u/v$ to approximate new atom values.
This requires separation.

\subsection{Summary of Assumptions (Three Tiers)}

For the representation theorem, we have three tiers of assumptions:

\textbf{Tier 1 (Base axioms---always required):}
\begin{enumerate}
\item \textbf{LinearOrder} (implicit in K\&S)
\item \textbf{Associativity, Identity, Strict Monotonicity} (K\&S Axioms 1--2)
\end{enumerate}

\textbf{Tier 2 (Regularity---one of the following):}
\begin{itemize}
\item \textbf{NoAnomalousPairs} (classical, used by H\"older approach)
\item \textbf{KSSeparation} (K\&S's implicit assumption, used by grid/cuts approaches)
\item \textbf{KSSeparation + KSSeparationStrict} (strongest, used by cuts approach)
\end{itemize}

\textbf{Relationship:}
\[
\text{KSSeparation} + \text{IdentIsMinimum} \;\Rightarrow\; \text{NoAnomalousPairs}
\]

From \texttt{NoAnomalousPairs} alone, we \emph{derive}:
\begin{itemize}
\item Commutativity (via Eric Paul's \texttt{not\_anomalous\_pair\_commutative})
\item H\"older embedding into $(\mathbb{R}, +)$ (via Eric's \texttt{holder\_not\_anom})
\item Additive representation $\Theta : \alpha \to \mathbb{R}$
\end{itemize}

\textbf{Key finding}: \texttt{NoAnomalousPairs} is sufficient for the representation theorem and connects to classical ordered semigroup theory.

\section{Philosophical Context: Cox, Kolmogorov, and K\&S}
\label{app:philosophy}

We compare three foundational approaches to probability, all formalized in Mettapedia.

\subsection{Kolmogorov (1933): Measure-Theoretic}

Kolmogorov's axioms define probability as a normalized measure on a $\sigma$-algebra.
Formalized in Mathlib as \texttt{IsProbabilityMeasure}:

\begin{lstlisting}[caption={Kolmogorov probability in Mathlib}]
class IsProbabilityMeasure (μ : Measure Ω) : Prop where
  measure_univ : μ Set.univ = 1
\end{lstlisting}

\textbf{Key properties} (all in Mathlib):
\begin{itemize}
\item $\sigma$-additivity: $\mu(\bigcup_i A_i) = \sum_i \mu(A_i)$ for disjoint $A_i$
\item Monotonicity: $A \subseteq B \Rightarrow \mu(A) \leq \mu(B)$
\item Normalization: $\mu(\Omega) = 1$, $\mu(\emptyset) = 0$
\end{itemize}

\textbf{Comparison with K\&S}: Kolmogorov \emph{axiomatizes} additivity; K\&S \emph{derives} it from associativity symmetry.

\subsection{Cox (1946, 1961): Logical Plausibility}

\textbf{Formalization}: See \texttt{Mettapedia/ProbabilityTheory/Cox/Basic.lean}

\subsubsection{Philosophical Setting}

Cox takes an \emph{epistemological} starting point: How should a rational agent assign degrees of belief to propositions?
Unlike Kolmogorov (who starts with sets and measures), Cox starts with propositions and asks what constraints logic imposes on plausibility assignments.

The key philosophical move: rather than axiomatizing what probability \emph{is}, Cox axiomatizes how plausibility \emph{behaves} and proves probability emerges uniquely.

\subsubsection{The Cox Axioms}

\begin{enumerate}
\item \textbf{Real-valued plausibility}: Each proposition $A$ given background $B$ has a plausibility $p(A|B) \in [0,1]$.

\item \textbf{Order preservation}: If $A$ is more plausible than $A'$ given $B$, then $p(A|B) > p(A'|B)$.

\item \textbf{Product rule} (the key axiom): There exists \emph{some} function $F$ such that:
\[p(A \land B | C) = F(p(A|C), p(B|A \land C))\]

\item \textbf{Negation rule}: There exists \emph{some} function $G$ such that:
\[p(\neg A | B) = G(p(A|B))\]
\end{enumerate}

The crucial point: Cox does \emph{not} assume $F(x,y) = xy$ or $G(x) = 1-x$.
He proves these are the \emph{only} possibilities (up to reparametrization).

\subsubsection{The Derivation}

The associativity of conjunction---$(A \land B) \land C \equiv A \land (B \land C)$---forces:
\[F(F(x,y), z) = F(x, F(y,z))\]

This is the same functional equation as K\&S Axiom 2!
Combined with \textbf{continuity} of $F$, it forces $F$ to be multiplication after a monotone reparametrization $\Theta$:
\[F(x,y) = \Theta^{-1}(\Theta(x) \cdot \Theta(y))\]

The standard choice $\Theta = \text{id}$ gives the probability product rule $p(A \land B | C) = p(A|C) \cdot p(B|A \land C)$.

\subsubsection{Continuity vs.\ Separation: A Key Contrast}

Both Cox and K\&S face the same challenge: the associativity equation $F(F(x,y),z) = F(x,F(y,z))$ has pathological solutions (Cauchy's discontinuous additive functions).
They rule these out differently:

\begin{center}
\begin{tabular}{lll}
& \textbf{Cox} & \textbf{K\&S} \\
\hline
Setting & $F : \mathbb{R} \to \mathbb{R} \to \mathbb{R}$ & Abstract algebra $\alpha$ \\
Regularity & $F$ is \textbf{continuous} & \textbf{Separation} property \\
Domain & Already on $\mathbb{R}$ & Must embed into $\mathbb{R}$ \\
\end{tabular}
\end{center}

\textbf{What they share}: Both conditions rule out pathological non-measurable solutions.

\textbf{How they differ}:
\begin{itemize}
\item \textbf{Continuity} (Cox): A topological condition on $\mathbb{R}$.
  Directly invokes the topology of the reals.

\item \textbf{Separation} (K\&S): An algebraic condition on abstract $\alpha$.
  For any $x < y$, there exist $n, m$ with $x^m < a^n \leq y^m$.
  This is \emph{density of rational powers}---no topology needed!
\end{itemize}

\textbf{Key insight}: Separation and continuity are \emph{similar conditions in different settings}.

\textbf{The precise relationship}:
\begin{enumerate}
\item \textbf{From $\alpha$ to $\mathbb{R}$}: Separation + base axioms $\Rightarrow$ embedding $\Theta : \alpha \to \mathbb{R}$ exists.
  Once embedded, the operation becomes addition on $\mathrm{Im}(\Theta) \subseteq \mathbb{R}$, which is automatically continuous.

\item \textbf{On $\mathbb{R}$}: Any continuous, monotone, associative $F : \mathbb{R} \to \mathbb{R} \to \mathbb{R}$ satisfies separation.
  Why? Separation on $(\mathbb{R}, +)$ says: for $x < y$, $\exists n,m$ with $mx < na \leq my$.
  This is just $x < (n/m) \cdot a \leq y$ for some rational---density of $\mathbb{Q} \cdot a$ in $\mathbb{R}$.

\item \textbf{Failure mode}: What if separation \emph{fails}? Then no embedding into $\mathbb{R}$ exists at all.
\end{enumerate}

\textbf{Counterexample}: \texttt{NatProdLex} ($\mathbb{N} \times \mathbb{N}$ with lexicographic order and componentwise addition) satisfies the base K\&S axioms but \emph{fails} separation.
The gap between $(0, k)$ and $(1, 0)$ cannot be bridged by rational powers.
Consequently, no order-preserving additive embedding into $\mathbb{R}$ exists.

See: \texttt{Additive/Counterexamples/} \\
\hspace*{2em} \texttt{Additive/Counterexamples/ProductFailsSeparation.lean}

\textbf{Connection theorem}: \texttt{cox\_comm\_of\_additiveOrderIsoRep} in \\
\hspace*{2em} \texttt{Literature/CoxTheorem.lean}

\subsection{K\&S (2012): Lattice Symmetries}

K\&S unify both approaches via lattice symmetries:
\begin{itemize}
\item Like Kolmogorov: works with lattices (Boolean for sets, more general for logic)
\item Like Cox: derives rules from functional equations, not axiomatizes them
\item Beyond both: minimal assumptions (no continuity, no differentiability)
\end{itemize}

\textbf{Summary of relationships}:
\begin{center}
\begin{tabular}{lccc}
& Kolmogorov & Cox & K\&S \\
\hline
Additivity & Axiom & Derived & Derived \\
Commutativity & Assumed & Derived & Derived \\
Continuity & Needed & Assumed & Not needed \\
Domain & $\sigma$-algebra & Propositions & Lattice \\
\end{tabular}
\end{center}

\section{The Direct Cuts Approach}
\label{app:directcuts}

K\&S prove the representation theorem via \textbf{inductive extension}: given a representation for $n$ atoms, extend it to $n+1$ atoms by finding the unique real value for the new atom.

Our formalization provides \textbf{both} proofs:
\begin{enumerate}
\item \textbf{K\&S's approach} (grid/induction): \texttt{Additive/Proofs/GridInduction/Main.lean} \\
  Uses \texttt{RepresentationGlobalization} typeclass machinery.

\item \textbf{Direct cuts} (our alternative): \texttt{Additive/Proofs/DirectCuts/Main.lean} \\
  Constructs $\Theta$ via Dedekind cuts explicitly. Smaller and more direct.
\end{enumerate}

Both prove the same theorem; the cuts version is more concise.

\subsection{Connection to Hölder and Alimov}

The representation theorem fits within a classical tradition of embedding theorems for ordered algebraic structures:

\begin{center}
\begin{tabular}{lll}
\textbf{Theorem} & \textbf{Structure} & \textbf{Result} \\
\hline
H\"older (1901) & Archimedean ordered abelian groups & Embed in $(\mathbb{R}, +)$ \\
Alimov (1950) & Cancellative ordered semigroups with NAP & Commutative + embed in $(\mathbb{R}, +)$ \\
Fuchs (1963) & Textbook treatment of the above & Classical reference \\
\textbf{Paul (2024)} & \textbf{Lean 4 formalization of the above} & \textbf{OrderedSemigroups} \\
K\&S + Separation & Ordered monoids with separation & Embed in $(\mathbb{R}, +)$ \\
\end{tabular}
\end{center}

\textbf{Key insight}: Eric Paul's OrderedSemigroups library (github.com/ericluap/OrderedSemigroups) provides a \textbf{complete Lean 4 formalization} of the Alimov-H\"older embedding theorem:

\begin{lstlisting}[caption={Eric Paul's H\"older theorem (from OrderedSemigroups)}]
theorem holder_not_anom [LinearOrder α] [Semigroup α]
    [IsOrderedCancelSemigroup α] [Pow α ℕ+] [PNatPowAssoc α]
    (h : ¬has_anomalous_pair (α := α)) :
    ∃ G : Subsemigroup (Multiplicative ℝ), Nonempty (α ≃*o G)
\end{lstlisting}

This theorem states: if a linearly ordered cancellative semigroup has no anomalous pairs, it embeds (as an ordered multiplicative isomorphism) into $(\mathbb{R}, +)$.

\textbf{Our contribution}: We show that \texttt{KnuthSkillingAlgebraBase} satisfies all the hypotheses of Eric's theorem:
\begin{itemize}
\item \texttt{Semigroup}: $\text{mul} := \text{op}$, $\text{mul\_assoc} := \text{op\_assoc}$
\item \texttt{IsOrderedCancelSemigroup}: From \texttt{op\_strictMono\_left/right}
\item \texttt{Pow \(\mathbb{N}^+\)}: $x^n := \text{iterate\_op}\, x\, n$
\item \texttt{PNatPowAssoc}: Requires commutativity, which Eric proves follows from $\neg\text{has\_anomalous\_pair}$!
\end{itemize}

Thus, \texttt{NoAnomalousPairs} suffices for the representation theorem.

\textbf{Files}:
\begin{itemize}
\item \texttt{Additive/Proofs/OrderedSemigroupEmbedding/HolderEmbedding.lean}: Bridge to Eric's library
\item \texttt{Additive/Axioms/AnomalousPairs.lean}: NoAnomalousPairs definition
\item \texttt{Additive/Proofs/GridInduction/Comparison.lean}: Unified interface
\end{itemize}

\subsection{The A/B/C Partition}

K\&S partition grid points when extending the representation to a new atom $d$:
\begin{itemize}
\item \textbf{Set A}: Points below some power of $d$: $\mu(F, r) < d^u$
\item \textbf{Set B}: Points equal to some power of $d$: $\mu(F, r) = d^u$
\item \textbf{Set C}: Points above some power of $d$: $\mu(F, r) > d^u$
\end{itemize}

\subsection{Dedekind Cut Property (Our Reformulation)}

When $B = \emptyset$ (the generic case), we show that $\delta := \Theta(d)$ is determined as a Dedekind cut:

\begin{lstlisting}[caption={Dedekind cut tightness lemma}]
lemma delta_cut_tight (hB_empty : ∀ r u, r ∉ extensionSetB F d u) :
    ∀ ε > 0, ∃ rA rC uA uC,
      rA ∈ extensionSetA F d uA ∧
      rC ∈ extensionSetC F d uC ∧
      |stat(rC, uC) - δ| < ε ∧
      |δ - stat(rA, uA)| < ε
\end{lstlisting}

\textbf{Meaning}: $\delta = \sup(\text{A-statistics}) = \inf(\text{C-statistics})$, a tight Dedekind cut.

\subsection{Why This Works}

The separation property (\texttt{KSSeparation}) guarantees that for any $x < y$, we can find exponents that ``separate'' them.
This provides the density needed for the Dedekind cut to uniquely determine $\delta$.

\textbf{Key insight}: The K\&S proof is essentially constructing Dedekind cuts to embed the abstract algebra into $\mathbb{R}$, one atom at a time.

\section{The Probability Hypercube}
\label{app:hypercube}

We formalize a multi-axis classification of probability theories, placing K\&S among its neighbors.

\subsection{Hypercube Axes}

Each axis represents a binary or multi-valued property of probability theories.

\textbf{Formalization}: See \texttt{Mettapedia/ProbabilityTheory/Hypercube/Basic.lean}

\begin{itemize}
\item \textbf{Commutativity}: $P(A \cap B) = P(B \cap A)$? (Classical: yes; Quantum: no)
\item \textbf{Precision}: Point-valued or interval-valued plausibilities?
\item \textbf{Additivity}: $P(A \cup B) = P(A) + P(B)$ for disjoint $A, B$?
\item \textbf{Lattice structure}: Boolean, orthomodular, distributive?
\item \textbf{Separation}: Can distinct values be separated by rationals?
\end{itemize}

\subsection{Named Vertices}

Key probability theories correspond to vertices of this hypercube:

\begin{center}
\begin{tabular}{llccccc}
\textbf{Framework} & \textbf{Result} & \textbf{Comm.} & \textbf{Precise} & \textbf{Add.} & \textbf{Lattice} & \textbf{Method} \\
\hline
Kolmogorov & Classical prob.\ & Yes & Yes & Axiom & Boolean & Axiom \\
Cox & Classical prob.\ & Yes & Yes & Derived & Boolean & Continuity \\
K\&S (Boolean) & Classical prob.\ & Yes & Yes & Derived & Boolean & Separation \\
\hline
K\&S (general) & Lattice plaus.\ & Yes & Yes & Derived & Distrib.\ & Separation \\
\hline
de Finetti & Subjective prob.\ & Yes & Yes & Finite & Boolean & Coherence \\
Dempster-Shafer & Belief functions & Yes & No & Sub & Boolean & Combination \\
Walley & Imprecise prob.\ & Yes & No & Sub & Boolean & Coherence \\
\hline
von Neumann & Quantum prob.\ & No & Yes & Axiom & Orthomod.\ & Axiom \\
\end{tabular}
\end{center}

\textbf{Key insight}: Kolmogorov, Cox, and K\&S are \emph{different vertices} (different derivation methods) that \emph{converge} to the same theory on Boolean lattices.
They differ in what they assume vs.\ derive:
\begin{itemize}
\item \textbf{Kolmogorov}: Axiomatizes additivity directly (no derivation)
\item \textbf{Cox}: Derives additivity from continuity of plausibility functions
\item \textbf{K\&S}: Derives additivity from separation (no continuity needed)
\end{itemize}

\subsection{Distributive vs.\ Boolean: What's Different?}

K\&S on general distributive lattices differs from classical probability in one key way: \textbf{no negation required}.

\begin{itemize}
\item \textbf{Boolean lattice}: Every element $a$ has a complement $\bar{a}$ with $a \vee \bar{a} = \top$ and $a \wedge \bar{a} = \bot$.
  This gives the ``negation'' rule $P(\bar{A}) = 1 - P(A)$.

\item \textbf{Distributive lattice}: No complements required.
  K\&S axioms never mention negation---this is the ``first fruit of our minimalist approach'' (K\&S).
\end{itemize}

\textbf{When does this matter?} Problems where not all combinations of states are allowed.
Rather than ``padding'' a distributive lattice to Boolean, K\&S works directly on the natural structure.

\textbf{Literature}: Valuations on distributive lattices appear in Klain \& Rota's \emph{Introduction to Geometric Probability} (cited by K\&S) and the theory of lattice valuations in combinatorics.

\textbf{Legend}:
\begin{itemize}
\item \textbf{Comm.}: Is the conjunction operation commutative? ($P(A \cap B) = P(B \cap A)$)
\item \textbf{Precise}: Single-valued (Yes) vs.\ interval-valued credal sets (No)
\item \textbf{Additive}: Full $\sigma$-additivity (Yes), finite additivity (Finite), or subadditivity (Sub)
\item \textbf{Lattice}: Boolean ($\sigma$-algebra), distributive (K\&S), or orthomodular (quantum)
\end{itemize}

\subsection{Novel Theories}

The hypercube suggests unexplored combinations:
\begin{itemize}
\item \textbf{Imprecise K\&S}: K\&S with interval-valued plausibilities (credal sets)
\item \textbf{Quantum D-S}: Dempster-Shafer on orthomodular lattices
\item \textbf{Non-commutative classical}: Classical probability without $P(A \cap B) = P(B \cap A)$
\end{itemize}

\textbf{Formalization}: See \texttt{Mettapedia/ProbabilityTheory/Hypercube/NovelTheories.lean} and \\
\hspace*{2em} \texttt{Mettapedia/ProbabilityTheory/Hypercube/NeighborTheories.lean}

\section{To-Do: Open Gaps in Formalization}
\label{app:todo}

\subsection{Event Lattice to Plausibility Scale}

\textbf{Status}: resolved.

The bridge is implemented in \texttt{Bridges/Model.lean} as:

\begin{lstlisting}
structure KSModel (E S : Type*)
    [PlausibilitySpace E] [KnuthSkillingAlgebraBase S] where
  v : E → S
  mono : Monotone v
  v_bot : v ⊥ = ident
  v_sup_of_disjoint : ∀ {a b}, Disjoint a b →
    v (a ⊔ b) = op (v a) (v b)
\end{lstlisting}

The scale $S$ is equipped with its K\&S algebra structure from the start.
The representation theorem tells us this algebra embeds into $(\mathbb{R}, +)$, so using $\mathbb{R}_{\geq 0}$ with addition as our scale is canonical.

\texttt{KSModelWithRepresentation} extends this by composing with the representation $\Theta : S \to \mathbb{R}$, yielding the full pipeline:
\[
E \xrightarrow{v} S \xrightarrow{\Theta} \mathbb{R}
\]

Concrete example: \texttt{Examples/CoinDie.lean} includes a fair coin flip and die roll as \texttt{ProbDist} sanity checks.

\subsection{Appendix B: Product Theorem}

\textbf{Status}: COMPLETE (no \texttt{sorry}, no \texttt{axiom} smuggling).

Lean proves that any $\otimes : \mathbb{R}_{>0}\to\mathbb{R}_{>0}\to\mathbb{R}_{>0}$ satisfying
distributivity over $+$ (Axiom~3) and mild regularity (associativity, plus injectivity of
$t \mapsto 1 \otimes t$) is multiplication up to a global scale constant.
This is formalized as \texttt{TensorRegularity} in
\texttt{Multiplicative/AczelTheorem.lean}.

\textbf{WARNING: Circular Reasoning Anti-Pattern}.
A previous file \texttt{EventBridge.lean} was \textbf{deleted} because it contained circular reasoning.
It claimed to ``bridge'' event-level to scalar-level by defining \texttt{mulTensor := multiplication}
and then ``proving'' multiplication equals scaled multiplication.  \textbf{This proves nothing!}

The error was confusing ``multiplication satisfies these axioms'' with ``any tensor satisfying these
axioms equals multiplication.''  The former is trivial field theory; the latter is the actual theorem.

The \textbf{correct approach} (what \texttt{AczelTheorem.lean} does):
\begin{enumerate}
\item Start with an \emph{abstract} tensor $\otimes$ satisfying K\&S Axioms 3--4
\item Prove that ANY such tensor must equal scaled multiplication
\item The tensor's identity is \emph{derived}, not assumed
\end{enumerate}

\subsection{Cauchy's Functional Equation}

\textbf{Status}: proved and used.

We prove the needed Cauchy lemma as
\texttt{continuous\_additive\_eq\_mul} in
\texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Multiplicative/FunctionalEquation.lean},
using Mathlib’s continuous additive $\Rightarrow$ $\mathbb{R}$-linear infrastructure.

\subsection{Concrete Examples}

\textbf{Status}: demonstrated.

See \texttt{Mettapedia/ProbabilityTheory/KnuthSkilling/Examples/} (and the nearby \texttt{Bridges/} and \texttt{Exploration/} folders) for:
\begin{itemize}
\item \texttt{Examples/CoinDie.lean}: Fair coin flip and fair die roll \texttt{ProbDist} examples.
  Verifies: $P(\emptyset)=0$, $P(\Omega)=1$, additivity $P(H \cup T) = P(H) + P(T)$.
\item \texttt{Bridges/Model.lean}: The three-element chain $\{\bot < m < \top\}$ as a \texttt{PlausibilitySpace}.
\item \texttt{Exploration/OpenIntervalLattice.lean}: Open sets / intervals of $\mathbb{R}$ as a distributive lattice.
\end{itemize}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{KnuthSkilling2012}
Kevin H. Knuth and John Skilling.
\textit{Foundations of Inference}.
Axioms, 1(1):38--73, 2012.

\bibitem{Paul2024}
Eric Paul.
\textit{OrderedSemigroups: Formalization in Lean 4}.
GitHub repository, 2024.
\url{https://github.com/ericluap/OrderedSemigroups}
See also: \url{https://ericluap.github.io/}

\bibitem{Alimov1950}
N.~G. Alimov.
On ordered semigroups.
\textit{Izv. Akad. Nauk SSSR Ser. Mat.}, 14:569--576, 1950.

\bibitem{Fuchs1963}
L.~Fuchs.
\textit{Partially Ordered Algebraic Systems}.
Pergamon Press, 1963.

\bibitem{Holder1901}
O.~H\"older.
Die Axiome der Quantit\"at und die Lehre vom Mass.
\textit{Berichte \"uber die Verhandlungen der K\"oniglich S\"achsischen Gesellschaft der Wissenschaften zu Leipzig, Mathematisch-Physische Classe}, 53:1--64, 1901.

\bibitem{Binder2016}
D.~Binder.
Non-Anomalous Semigroups and Real Numbers.
arXiv:1607.05997, 2016.

\bibitem{RockafellarWets}
R.~T. Rockafellar and R.~J.-B. Wets.
\textit{Variational Analysis}.
Springer, 1998.

\bibitem{Clarke1990}
F.~H. Clarke.
\textit{Optimization and Nonsmooth Analysis}.
SIAM, 1990.

\end{thebibliography}

\end{document}
