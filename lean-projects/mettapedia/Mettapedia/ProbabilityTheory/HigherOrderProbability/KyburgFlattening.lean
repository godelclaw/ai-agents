import Mettapedia.ProbabilityTheory.HigherOrderProbability.Basic
import Mathlib.MeasureTheory.Integral.Bochner.Basic

/-!
# Kyburg's Flattening Theorem

**Status**: Week 2 - Main Theorem üöß
**Dependencies**: Basic.lean (Week 1 ‚úÖ)

This file formalizes Kyburg's key result from "Higher Order Probabilities" (1988):

> "Higher-order probabilities can always be replaced by marginal distributions
> of joint probability distributions."

## Mathematical Content

Given a parametrized distribution (kernel + mixing measure), we prove:
1. **Flattening preserves predictions**: Marginalizing the joint recovers the mixture
2. **Expectation consistency**: E[U] under flattened = E[E[U|Œ∏]]
3. **No computational advantage**: Decisions using joint = decisions using flattened

## Main Theorems

* `kyburg_flattening` : The core reduction theorem
* `expectation_consistency` : Expected utilities are preserved
* `kyburg_no_advantage` : Decision-theoretic equivalence

## Proof Strategy

1. Start with definition of flattening (from Basic.lean)
2. Use Fubini's theorem to interchange integration
3. Show marginalization property: ‚à´_Œò P(Œ∏, x) dŒ∏ = P(x)
4. Extend to expectation of utilities

## References

- Kyburg, H.E. (1988). "Higher Order Probabilities". Technical Report.
- This is the multiplication/join operation of the Giry monad

-/

namespace Mettapedia.ProbabilityTheory.HigherOrderProbability

open MeasureTheory ProbabilityTheory ParametrizedDistribution
open scoped ENNReal

variable {Œò X : Type*} [MeasurableSpace Œò] [MeasurableSpace X]

/-! ## The Main Theorem -/

/-- **Kyburg's Flattening Theorem** (1988).

Given a "second-order" probability represented as a parametrized distribution,
marginalizing the joint distribution P(Œ∏, x) = Œº(Œ∏) ¬∑ kernel(Œ∏)(x) over Œò
recovers the flattened distribution that makes the same predictions on X.

**Interpretation**: "Higher-order probabilities offer no computational or conceptual
advantage" - they can always be replaced by a joint distribution whose marginals
give the same predictions.

**Formal Statement**: For any measurable set A ‚äÜ X,
  P_flattened(A) = ‚à´_Œò P_Œ∏(A) dŒº(Œ∏)

This says: the probability of A under the flattened distribution equals the expected
probability of A under the parametrized family, weighted by the mixing measure.
-/
theorem kyburg_flattening (pd : ParametrizedDistribution Œò X) (A : Set X)
    (hA : MeasurableSet A) :
    (flatten pd) A = ‚à´‚Åª Œ∏, (pd.kernel Œ∏) A ‚àÇpd.mixingMeasure := by
  -- This is exactly flatten_apply from Basic.lean
  exact flatten_apply pd A hA

/-! ## Corollaries and Extensions -/

/-- The flattening is the marginal of the joint over X.

This makes explicit that Kyburg's construction satisfies the classical
marginalization property: P(x) = ‚à´_Œò P(Œ∏, x) dŒ∏.
-/
theorem flatten_is_marginal (pd : ParametrizedDistribution Œò X) :
    flatten pd = (kyburgJoint pd).map Prod.snd := by
  rw [flatten_eq_snd_kyburgJoint]
  -- snd is defined as map Prod.snd
  rfl

/-- **Expectation Consistency** (Kyburg's Expectation Condition).

The expectation of a utility U under the flattened distribution equals
the "meta-expectation": first compute E[U|Œ∏] for each Œ∏, then average
these conditional expectations weighted by Œº.

Formally: E_flattened[U] = E_Œº[E_Œ∏[U]]

This is the key to Kyburg's "no advantage" result: any decision problem
can be solved using just the flattened distribution.
-/
theorem expectation_consistency (pd : ParametrizedDistribution Œò X)
    (U : X ‚Üí ‚Ñù‚â•0‚àû) (hU : Measurable U) :
    ‚à´‚Åª x, U x ‚àÇ(flatten pd) = ‚à´‚Åª Œ∏, (‚à´‚Åª x, U x ‚àÇ(pd.kernel Œ∏)) ‚àÇpd.mixingMeasure := by
  unfold flatten
  rw [Measure.lintegral_bind pd.kernel.aemeasurable hU.aemeasurable]

/-! ## Decision-Theoretic Equivalence -/

/-- **Kyburg's No-Advantage Theorem** (informal version).

For any decision problem with utilities U(action, x), the optimal action
is the same whether we:
- Use the full joint P(Œ∏, x) and integrate out Œ∏
- Use the flattened distribution P(x) directly

This formalizes Kyburg's conclusion: "there is no conceptual advantage to
representing [probabilities] as first and second order as opposed to joint."

We prove this for expected utilities (the foundation of decision theory).
-/
theorem kyburg_no_advantage {Action : Type*} (pd : ParametrizedDistribution Œò X)
    (U : Action ‚Üí X ‚Üí ‚Ñù‚â•0‚àû) (hU : ‚àÄ a, Measurable (U a)) (a : Action) :
    ‚à´‚Åª x, U a x ‚àÇ(flatten pd) =
    ‚à´‚Åª Œ∏, (‚à´‚Åª x, U a x ‚àÇ(pd.kernel Œ∏)) ‚àÇpd.mixingMeasure := by
  exact expectation_consistency pd (U a) (hU a)

/-! ## Special Cases and Examples -/

section FiniteSpaces

variable (Œò_fin : Type*) [Fintype Œò_fin] [MeasurableSpace Œò_fin]
  [MeasurableSingletonClass Œò_fin]

/-- In the finite case, the flattening formula becomes a finite sum.

This is pedagogically useful: Kyburg's reduction is just a weighted average
when both parameter space and observation space are finite.
-/
theorem kyburg_flattening_finite (pd : ParametrizedDistribution Œò_fin X)
    (A : Set X) (hA : MeasurableSet A) :
    (flatten pd) A = ‚àë' Œ∏ : Œò_fin, (pd.mixingMeasure {Œ∏}) * (pd.kernel Œ∏ A) := by
  -- This follows from kyburg_flattening; the finite case is just notation
  -- For fintype, ‚àë and ‚àë' are equivalent
  simp [kyburg_flattening pd A hA, lintegral_fintype, mul_comm]

end FiniteSpaces

/-! ## Connection to Probability Monad -/

/-- Flattening is the monadic join/multiplication operation.

In categorical terms, the Giry monad has:
- Unit: Œ∑(x) = Œ¥_x (Dirac measure)
- Multiplication: Œº ‚ãÜ (Œò ‚Üí Measure X) ‚Ü¶ flatten(Œº, kernel)

Kyburg's theorem says this multiplication is well-defined and gives
the "right" semantics for higher-order probability.
-/
theorem flatten_is_monad_multiplication (pd : ParametrizedDistribution Œò X) :
    flatten pd = pd.mixingMeasure.bind pd.kernel := by
  rfl  -- This is definitional

/-! ## Remarks

1. **Sufficient for Decision Theory**: The theorem shows that for any utility function,
   decisions made with the flattened distribution are identical to decisions made with
   the full joint distribution.

2. **Computational Efficiency**: In practice, storing just the flattened distribution
   can be much more efficient than maintaining the full joint, especially when Œò is
   high-dimensional.

3. **Connection to PLN**: PLN's evidence (n‚Å∫, n‚Åª) is exactly the sufficient statistic
   for a Beta-Bernoulli Kyburg flattening. This will be formalized in
   Logic/HigherOrder/PLNKyburgReduction.lean (Phase 2).

4. **De Finetti Connection**: De Finetti's representation theorem is a special case
   where Œò = [0,1] and kernel(Œ∏) = Bernoulli(Œ∏). This will be formalized in
   DeFinettiConnection.lean (Week 3).
-/

end Mettapedia.ProbabilityTheory.HigherOrderProbability
